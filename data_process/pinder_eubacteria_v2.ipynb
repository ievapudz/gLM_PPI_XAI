{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e372a53-282c-4c38-b24b-e96982434ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pinder.core import get_index, PinderSystem\n",
    "from pinder.core import PinderLoader\n",
    "from pinder.core.loader import filters\n",
    "from pinder.core.loader import structure\n",
    "from pinder.core.index.system import PinderSystem\n",
    "from pinder.core import get_pinder_location\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "TAX_LEVEL=\"species\"\n",
    "\n",
    "HOME=\"/scicore/home/schwede/pudziu0000/\"\n",
    "PINDER_BASE_DIR=\"/scicore/home/schwede/durair0000/.local/share/\"\n",
    "UNIPROT_IDS_PATH=f\"{HOME}/projects/gLM/data/PINDER/uniprot_ids_1024_512.lst\"\n",
    "UNIPROT_LINEAGES_PATH=f\"{HOME}/projects/gLM/data/PINDER/uniprot_lineages_annotations_1024_512.tsv\"\n",
    "SPLITS_DIR=f\"{HOME}/projects/gLM/data/PINDER/eubacteria_5_1024_512_{TAX_LEVEL}/\"\n",
    "SEED=1024\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModel.from_pretrained(f\"{HOME}/projects/gLM/gLM2_650M\", torch_dtype=torch.bfloat16, trust_remote_code=True).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{HOME}/projects/gLM/gLM2_650M\", trust_remote_code=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def select_ids(max_length=1024, max_length_per_monomer=400):\n",
    "    base_filters = [\n",
    "        filters.FilterByMissingHolo(),\n",
    "        filters.FilterSubByContacts(min_contacts=5, radius=10.0, calpha_only=True),\n",
    "        filters.FilterDetachedHolo(radius=12, max_components=2),\n",
    "    ]\n",
    "    sub_filters = [\n",
    "        filters.FilterSubByAtomTypes(min_atom_types=4),\n",
    "        filters.FilterByHoloOverlap(min_overlap=5),\n",
    "        filters.FilterByHoloSeqIdentity(min_sequence_identity=0.8),\n",
    "        filters.FilterSubRmsds(rmsd_cutoff=7.5),\n",
    "        filters.FilterDetachedSub(radius=12, max_components=2),\n",
    "    ]\n",
    "    split_pinder_ids = {}\n",
    "    split_uniprot_ids = {}\n",
    "    train_index = None\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        loader = PinderLoader(\n",
    "            split=split,\n",
    "            monomer_priority=\"holo\",\n",
    "            base_filters=base_filters,\n",
    "            sub_filters=sub_filters,\n",
    "        )\n",
    "            \n",
    "        index = loader.index.merge(loader.metadata, on=\"id\", how=\"left\")\n",
    "        index[\"length\"] = index[\"length1\"] + index[\"length2\"]\n",
    "        index[\"resolution\"] = index[\"resolution\"].astype(\"float32\")\n",
    "        index = index[\n",
    "            (index[\"length\"] + 2 <= max_length)\n",
    "            & (index[\"length1\"] <= max_length_per_monomer)\n",
    "            & (index[\"length2\"] <= max_length_per_monomer)\n",
    "            & (index[\"label\"] == \"BIO\")\n",
    "        ]\n",
    "        if(split == \"train\"): \n",
    "            index = index[\n",
    "                (index[\"length\"] + 2 <= max_length)\n",
    "                & (index[\"length1\"] <= max_length_per_monomer)\n",
    "                & (index[\"length2\"] <= max_length_per_monomer)\n",
    "                & (index[\"label\"] == \"BIO\")\n",
    "                & (index[\"cluster_id_R\"] != index[\"cluster_id_L\"])  # Filter to keep only heterodimers in training\n",
    "            ]\n",
    "            train_index = index\n",
    "        pinder_ids = list(index[\"id\"])\n",
    "        uniprot_ids = set(list(index[\"uniprot_R\"]) + list(index[\"uniprot_L\"]))\n",
    "        split_pinder_ids[split] = pinder_ids\n",
    "        split_uniprot_ids[split] = uniprot_ids\n",
    "    return split_pinder_ids, split_uniprot_ids, train_index\n",
    "\n",
    "def reduce_train_redundancy(ids, index, max_per_cluster=5):\n",
    "    # ids - list of ids of the training set\n",
    "    # index - index of the training set\n",
    "    ids = list(\n",
    "        set(\n",
    "            index.sort_values(\n",
    "                [\"resolution\", \"id\"],\n",
    "                ascending=[True, True],\n",
    "            )\n",
    "            .groupby(\"cluster_id\", observed=False)\n",
    "            .head(max_per_cluster)[\"id\"]\n",
    "        )\n",
    "    )\n",
    "    return ids\n",
    "\n",
    "def get_positives_df(split, pinder_ids, pinder_tax_annot):\n",
    "    positives = []\n",
    "    for id_ in eubacteria_pinder_ids[split]:\n",
    "        record = {\n",
    "            \"pinder_id\": id_,\n",
    "            \"pinder_id_R\": id_.split(\"--\")[0], \n",
    "            \"pinder_id_L\": id_.split(\"--\")[1], \n",
    "            f\"{TAX_LEVEL}_R\": pinder_tax_annot[id_][0],\n",
    "            f\"{TAX_LEVEL}_L\": pinder_tax_annot[id_][1]}\n",
    "        positives.append(record)\n",
    "    return pd.DataFrame.from_dict(positives)\n",
    "\n",
    "def make_random_pairs(L, pdb_L, chain_uniprot_L, uniprot_L, \n",
    "                      pdb_R, uniprot_R,\n",
    "                      interacting_partners, GO, max_num_attempts):\n",
    "    invalidity_flags = []\n",
    "    # Shuffle until no same PDB pairs\n",
    "    for attempt in range(max_num_attempts):  # Limit reattempts\n",
    "        L_info = list(zip(L, pdb_L, chain_uniprot_L, uniprot_L))\n",
    "        if(len(L_info) == 0): return (None, None, None, None, None)\n",
    "        random.Random(SEED).shuffle(L_info)\n",
    "        L, pdb_L, chain_uniprot_L, uniprot_L = zip(*L_info)\n",
    "        \n",
    "        # If UniProt IDs of the new pair are known to contain interacting sequences, retry\n",
    "        if any(uL in interacting_partners[uR] for uR, uL in zip(uniprot_R, uniprot_L)):\n",
    "            if(attempt != max_num_attempts-1):\n",
    "                continue\n",
    "            else:\n",
    "                invalidity_flags.append(\"interacting\")\n",
    "                \n",
    "        # If UniProt IDs of the new pair have overlapping Molecular Function GOs, retry\n",
    "        if any(bool(set(GO[uR]) & set(GO[uL])) for uR, uL in zip(uniprot_R, uniprot_L)):\n",
    "            if(attempt != max_num_attempts-1):\n",
    "                continue\n",
    "            else:\n",
    "                invalidity_flags.append(\"GO\")\n",
    "\n",
    "        # If PDB IDs of all pairs are non-matching, call it a success\n",
    "        if all(pR != pL for pR, pL in zip(pdb_R, pdb_L)): break\n",
    "\n",
    "    return L, pdb_L, chain_uniprot_L, uniprot_L, invalidity_flags\n",
    "\n",
    "\n",
    "def remove_false_negatives(R, L, uniprot_R, uniprot_L, interacting_partners, \n",
    "                           GO, invalidity_flags, orig_pinder_ids):\n",
    "    # If there are invalid pairings, execute removals\n",
    "    if(invalidity_flags):\n",
    "        to_remove = set()\n",
    "        for i, (uR, uL) in enumerate(zip(uniprot_R, uniprot_L)):\n",
    "            if \"interacting\" in invalidity_flags and uL in interacting_partners[uR]: to_remove.add(i)\n",
    "            if \"GO\" in invalidity_flags and bool(set(GO[uR]) & set(GO[uL])): to_remove.add(i)\n",
    "        \n",
    "        valid_indices_R = set(range(len(R)))-to_remove\n",
    "        valid_indices_L = set(range(len(L)))-to_remove\n",
    "        R = [R[i] for i in valid_indices_R]\n",
    "        L = [L[i] for i in valid_indices_L]\n",
    "\n",
    "    # Forming new pinder_ids\n",
    "    paired = zip(R, L)\n",
    "    paired = [\"--\".join(pair) for pair in paired]\n",
    "    \n",
    "    # If new pinder_id is present in the original set, remove that record\n",
    "    paired = [item for item in paired if item not in orig_pinder_ids]\n",
    "\n",
    "    return paired\n",
    "\n",
    "def make_negative_pairs(dfR, dfL, interacting_partners, GO, seed=SEED, max_num_attempts=5, tax_level=\"phylum\"):    \n",
    "    # Separate ids\n",
    "    \n",
    "    dfR[['R', 'L']] = dfR['pinder_id'].str.split('--', expand=True)\n",
    "    dfR[['pdb_R', 'chain_uniprot_R']] = dfR['R'].str.split('__', expand=True)\n",
    "    dfR[['chain_R', 'uniprot_R']] = dfR['chain_uniprot_R'].str.split('_', expand=True)\n",
    "    dfR[['pdb_L', 'chain_uniprot_L']] = dfR['L'].str.split('__', expand=True)\n",
    "    dfR[['chain_L', 'uniprot_L']] = dfR['chain_uniprot_L'].str.split('_', expand=True)\n",
    "\n",
    "    dfL[['R', 'L']] = dfL['pinder_id'].str.split('--', expand=True)\n",
    "    dfL[['pdb_R', 'chain_uniprot_R']] = dfL['R'].str.split('__', expand=True)\n",
    "    dfL[['chain_R', 'uniprot_R']] = dfL['chain_uniprot_R'].str.split('_', expand=True)\n",
    "    dfL[['pdb_L', 'chain_uniprot_L']] = dfL['L'].str.split('__', expand=True)\n",
    "    dfL[['chain_L', 'uniprot_L']] = dfL['chain_uniprot_L'].str.split('_', expand=True)\n",
    "    \n",
    "    # Group by tax_level\n",
    "    negative_ids = []\n",
    "    negative_phyla = []\n",
    "    orig_pinder_ids = set(dfR['pinder_id'])\n",
    "    \n",
    "    for tax_level_el, group in dfR.groupby(f'{tax_level}_R'):\n",
    "        R = group['R'].tolist()\n",
    "        pdb_R = group['pdb_R'].tolist()\n",
    "        uniprot_R = group['uniprot_R'].tolist()\n",
    "\n",
    "        group_L = dfL[dfL[f'{tax_level}_L'] == tax_level_el]\n",
    "        L = group_L['L'].tolist()\n",
    "        pdb_L = group_L['pdb_L'].tolist()\n",
    "        chain_uniprot_L = group_L['chain_uniprot_L'].tolist()\n",
    "        uniprot_L = group_L['uniprot_L'].tolist()\n",
    "\n",
    "        L, pdb_L, chain_uniprot_L, uniprot_L, invalidity_flags = make_random_pairs(\n",
    "            L, pdb_L, chain_uniprot_L, uniprot_L, pdb_R, uniprot_R, \n",
    "            interacting_partners, GO, max_num_attempts\n",
    "        )\n",
    "        # If there are no pairs to form\n",
    "        if(not L): continue\n",
    "        \n",
    "        # If there are invalid pairings, execute removals and get pairs\n",
    "        paired = remove_false_negatives(R, L, uniprot_R, uniprot_L, \n",
    "            interacting_partners, GO, invalidity_flags, orig_pinder_ids\n",
    "        )\n",
    "\n",
    "        negative_ids.extend(paired)\n",
    "        negative_phyla.extend([tax_level_el]*len(paired))\n",
    "\n",
    "    negative_records = list(zip(negative_ids, negative_phyla, negative_phyla))\n",
    "    negative_df = pd.DataFrame(\n",
    "        negative_records, \n",
    "        columns=['pinder_id', f'{tax_level}_R', f'{tax_level}_L']\n",
    "    ).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    negative_df[['pinder_id_R', 'pinder_id_L']] = negative_df['pinder_id'].str.split('--', expand=True)\n",
    "    \n",
    "    return negative_df\n",
    "\n",
    "def rename_columns(df, to_drop=None, tax_level=\"phylum\"):\n",
    "    df.rename(columns={\n",
    "        \"pinder_id_R\": \"protein1\",\n",
    "        \"pinder_id_L\": \"protein2\",\n",
    "        f\"{tax_level}_R\": f\"{tax_level}1\",\n",
    "        f\"{tax_level}_L\": f\"{tax_level}2\"\n",
    "    }, inplace=True)\n",
    "    if(to_drop): df.drop([to_drop], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_sequences(df):\n",
    "    ids = {\"R\": set(), \"L\": set()}\n",
    "    \n",
    "    for i, j in df.iterrows():\n",
    "        ids[\"R\"].add(df[\"pinder_id_R\"][i])\n",
    "        ids[\"L\"].add(df[\"pinder_id_L\"][i])\n",
    "        \n",
    "    seqs = {}\n",
    "    for k in ids:\n",
    "        for id_ in ids[k]:\n",
    "            struct = structure.Structure(f\"{PINDER_BASE_DIR}/pinder/2024-02/pdbs/{id_}-{k}.pdb\", pinder_id=id_)\n",
    "            seqs[id_] = struct.sequence\n",
    "    return seqs\n",
    "\n",
    "def plot_distributions(dist1, dist2, title, split):\n",
    "    matplotlib.rcParams['figure.dpi'] = 300\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    hist1, _ = np.histogram(dist1, density=True)\n",
    "    hist2, _ = np.histogram(dist2, density=True)\n",
    "    \n",
    "    plt.hist(dist1, color='red', alpha=0.5, label='positive')\n",
    "    plt.hist(dist2, color='blue', alpha=0.5, label='negative')\n",
    "    plt.xlabel('distance')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(f\"{HOME}/projects/gLM/data/PINDER/{split}.png\", dpi=200)\n",
    "    plt.clf()\n",
    "    \n",
    "    return hist1, hist2\n",
    "\n",
    "def compute_KL_divergence(hist1, hist2):\n",
    "    # Clip to avoid division by zero and ensure valid inputs\n",
    "    epsilon = 1e-10\n",
    "    p1 = np.clip(hist1, epsilon, None)\n",
    "    p2 = np.clip(hist2, epsilon, None)\n",
    "    \n",
    "    # Normalize histograms to make them proper probability distributions\n",
    "    p1 /= p1.sum()\n",
    "    p2 /= p2.sum()\n",
    "    \n",
    "    kl_12 = np.sum(rel_entr(p1, p2))\n",
    "    kl_21 = np.sum(rel_entr(p2, p1))\n",
    "    \n",
    "    return kl_12, kl_21\n",
    "\n",
    "def generate_embeddings(df, seq_df, model, tokenizer, num_samples=5, emb_dim=1280):\n",
    "    embs_R = torch.empty((1, num_samples, emb_dim), dtype=torch.float64)\n",
    "    embs_L = torch.empty((1, num_samples, emb_dim), dtype=torch.float64)\n",
    "\n",
    "    df = df.reset_index()\n",
    "    for i, record in df[:num_samples].iterrows():\n",
    "        enc_R = tokenizer([seq_df[record['pinder_id_R']]], return_tensors='pt').to(DEVICE)\n",
    "        enc_L = tokenizer([seq_df[record['pinder_id_L']]], return_tensors='pt').to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embs_R[0][i] = model(enc_R.input_ids, output_hidden_states=True).last_hidden_state.mean(dim=1)\n",
    "            embs_L[0][i] = model(enc_L.input_ids, output_hidden_states=True).last_hidden_state.mean(dim=1)\n",
    "            \n",
    "    return torch.squeeze(embs_R), torch.squeeze(embs_L)\n",
    "\n",
    "def make_split_fasta(split):\n",
    "    df = pd.read_csv(f\"{SPLITS_DIR}/{split}.txt\", sep=\"\\t\")\n",
    "\n",
    "    fasta = []\n",
    "    # Reducing duplicates\n",
    "    ids = {\"R\": set(), \"L\": set()}\n",
    "    for i, j in df.iterrows():\n",
    "        idR = df[\"protein1\"][i]\n",
    "        idL = df[\"protein2\"][i]\n",
    "        ids[\"R\"].add(idR)\n",
    "        ids[\"L\"].add(idL)\n",
    "\n",
    "    for k in ids:\n",
    "        for id_ in ids[k]:\n",
    "            struct = structure.Structure(f\"{PINDER_BASE_DIR}/pinder/2024-02/pdbs/{id_}-{k}.pdb\", pinder_id=id_)\n",
    "            fasta.append(SeqRecord(Seq(struct.sequence), id=id_, description=\"\"))\n",
    "    \n",
    "    with open(f\"{SPLITS_DIR}/{split}.fasta\", \"w\") as output_handle:\n",
    "        SeqIO.write(fasta, output_handle, \"fasta-2line\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de970895-ea39-4f2b-b294-80003d7d5ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/scicore/home/schwede/durair0000/.local/share/pinder/2024-02')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PINDER_BASE_DIR\"] = PINDER_BASE_DIR\n",
    "get_pinder_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f007fc0d-da6f-4e71-bac5-7b8cface420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinder_ids, uniprot_ids, train_index = select_ids(max_length_per_monomer=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f82601cf-96a0-48b5-a2c0-badaccf3e11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338759 1799 1767\n",
      "15116 1937 1906\n"
     ]
    }
   ],
   "source": [
    "# Checking number of elements in each split\n",
    "print(len(pinder_ids['train']),\n",
    "      len(pinder_ids['val']),\n",
    "      len(pinder_ids['test'])\n",
    ")\n",
    "\n",
    "print(len(uniprot_ids['train']),\n",
    "      len(uniprot_ids['val']),\n",
    "      len(uniprot_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df3af2a-b115-4ec8-8950-247b1a3de70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting that that both defined UniProt IDs\n",
    "pinder_ids['train'] = [row for row in pinder_ids['train'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "pinder_ids['val'] = [row for row in pinder_ids['val'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "pinder_ids['test'] = [row for row in pinder_ids['test'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "\n",
    "uniprot_ids['train'].remove(\"UNDEFINED\")\n",
    "uniprot_ids['val'].remove(\"UNDEFINED\")\n",
    "uniprot_ids['test'].remove(\"UNDEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ebdca60-6e15-4274-ae87-e8fdf22ef01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301263 1748 1688\n",
      "15115 1936 1905\n"
     ]
    }
   ],
   "source": [
    "# Checking number of elements in each split after removal of \"UNDEFINED\"\n",
    "print(len(pinder_ids['train']),\n",
    "      len(pinder_ids['val']),\n",
    "      len(pinder_ids['test'])\n",
    ")\n",
    "\n",
    "print(len(uniprot_ids['train']),\n",
    "      len(uniprot_ids['val']),\n",
    "      len(uniprot_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "073c00a5-62d8-4353-978b-2055d57b4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save UniProt IDs to a TXT file for \"ID mapping\"\n",
    "with open(UNIPROT_IDS_PATH, 'w') as f:\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for line in uniprot_ids[split]:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b0b7f998-e9ce-4018-9eed-9e1ebd5bd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually run the ID mapping on the UniProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67c3212f-9a63-4823-bfe3-2215349b46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine, which UniProt IDs are belonging to eubacteria with labelled tax_level\n",
    "lineages = pd.read_csv(UNIPROT_LINEAGES_PATH, sep='\\t')\n",
    "\n",
    "eubacteria_phyla = {}\n",
    "eubacteria_annotated_interactions = {}\n",
    "eubacteria_GO = {}\n",
    "\n",
    "for index, row in lineages.iterrows():\n",
    "    if isinstance(row['Taxonomic lineage (Ids)'], str) and \"2 (superkingdom)\" in row['Taxonomic lineage (Ids)']: \n",
    "        match = re.search(r'(\\d+) \\(%s\\)' % TAX_LEVEL, row['Taxonomic lineage (Ids)'])\n",
    "        if match:\n",
    "            tax_level_tax_id = match.group(1)\n",
    "            eubacteria_phyla[row['From']] = tax_level_tax_id\n",
    "            \n",
    "            # Retrieving UniProt IDs of the known interacting partners\n",
    "            if(not pd.isna(row['Interacts with'])):\n",
    "                eubacteria_annotated_interactions[row['From']] = [item.split('-')[0] for item in row['Interacts with'].split('; ')]\n",
    "            else:\n",
    "                eubacteria_annotated_interactions[row['From']] = []\n",
    "            \n",
    "            # Retrieving GO \n",
    "            if(not pd.isna(row['Gene Ontology (molecular function)'])):\n",
    "                eubacteria_GO[row['From']] = re.findall(r\"GO:\\d+\", row['Gene Ontology (molecular function)'])\n",
    "            else:\n",
    "                eubacteria_GO[row['From']] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284544b4-bae4-48a3-8087-f21504d0745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine, which PINDER IDs contain both bacterial proteins\n",
    "eubacteria_pinder_ids = {'train': [], 'val': [], 'test': []}\n",
    "eubacteria_pinder_phyla = {}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for id in pinder_ids[split]:\n",
    "        uniprot1 = id.split(\"--\")[0].split(\"_\")[-1]\n",
    "        uniprot2 = id.split(\"--\")[1].split(\"_\")[-1]\n",
    "        if(uniprot1 in eubacteria_phyla.keys() and uniprot2 in eubacteria_phyla.keys()):\n",
    "            eubacteria_pinder_ids[split].append(id)\n",
    "            eubacteria_pinder_phyla[id] = [eubacteria_phyla[uniprot1], eubacteria_phyla[uniprot2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bf36719-20e2-4059-941f-e34aa313d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17863 680 645\n"
     ]
    }
   ],
   "source": [
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87d70941-031b-4b6d-91ca-9bd44bb826eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train']:\n",
    "    homodimer_idx = []\n",
    "    for i, id_ in enumerate(eubacteria_pinder_ids[split]):\n",
    "        uniprot1 = id_.split(\"--\")[0].split(\"_\")[-1]\n",
    "        uniprot2 = id_.split(\"--\")[1].split(\"_\")[-1]\n",
    "        if(uniprot1 == uniprot2): homodimer_idx.append(i)\n",
    "\n",
    "    eubacteria_pinder_ids[split] = np.delete(\n",
    "        eubacteria_pinder_ids[split], \n",
    "        homodimer_idx\n",
    "    ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66847f77-5086-4d21-986c-8068aed95d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17409 680 645\n"
     ]
    }
   ],
   "source": [
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e0dfa7d-1c68-447b-a077-bba512ff2019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\tKL(1|0)\tKL(0|1)\n",
      "val\t4.021\t4.372\n",
      "test\t1.418\t1.775\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positives = {}\n",
    "negatives = {}\n",
    "pos_seq = {}\n",
    "neg_seq = {}\n",
    "pos_embs_R = {}\n",
    "pos_embs_L = {}\n",
    "neg_embs_R = {}\n",
    "neg_embs_L = {}\n",
    "pos_dists = {}\n",
    "neg_dists = {}\n",
    "hist_pos = {}\n",
    "hist_neg = {}\n",
    "\n",
    "n_samples = None\n",
    "\n",
    "print(f\"split\\tKL(1|0)\\tKL(0|1)\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    positives[split] = get_positives_df(split, eubacteria_pinder_ids, eubacteria_pinder_phyla)\n",
    "    \n",
    "    negatives[split] = make_negative_pairs(positives[split], \n",
    "        positives[\"train\"], eubacteria_annotated_interactions, \n",
    "        eubacteria_GO, tax_level=TAX_LEVEL\n",
    "    )\n",
    "\n",
    "    if(split == 'train'): continue\n",
    "\n",
    "    # Get sequences\n",
    "    pos_seq[split] = get_sequences(positives[split][:n_samples])\n",
    "    neg_seq[split] = get_sequences(negatives[split][:n_samples])\n",
    "\n",
    "    # Get embeddings\n",
    "    pos_embs_R[split], pos_embs_L[split] = generate_embeddings(\n",
    "        positives[split][:n_samples], pos_seq[split], model, tokenizer, \n",
    "        num_samples=len(positives[split][:n_samples]), emb_dim=1280\n",
    "    )\n",
    "    neg_embs_R[split], neg_embs_L[split] = generate_embeddings(\n",
    "        negatives[split][:n_samples], neg_seq[split], model, tokenizer, \n",
    "        num_samples=len(negatives[split][:n_samples]), emb_dim=1280\n",
    "    )\n",
    "\n",
    "    # Compute distances\n",
    "    pos_dists[split] = torch.cdist(\n",
    "        pos_embs_R[split], \n",
    "        pos_embs_L[split]\n",
    "    ).diag()\n",
    "    neg_dists[split] = torch.cdist(\n",
    "        neg_embs_R[split], \n",
    "        neg_embs_L[split]\n",
    "    ).diag()\n",
    "\n",
    "    # Plot distances distribution\n",
    "    hist_pos[split], hist_neg[split] = plot_distributions(\n",
    "        pos_dists[split], neg_dists[split], \n",
    "        f\"Distance distribution of {split} subset\", split\n",
    "    )\n",
    "\n",
    "    # Compute KL divergence\n",
    "    kl10, kl01 = compute_KL_divergence(hist_pos[split], hist_neg[split])\n",
    "    print(f\"{split}\\t{kl10:.3f}\\t{kl01:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "90b837a8-05e1-49a4-a43d-955d3ba74990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3144 2347 680 458 645 434\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(positives['train']),\n",
    "    len(negatives['train']),\n",
    "    len(positives['val']),\n",
    "    len(negatives['val']),\n",
    "    len(positives['test']),\n",
    "    len(negatives['test'])\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c897770-537a-442b-96d9-7c13bfd4984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundancy from the training set\n",
    "mask = train_index['id'].isin(eubacteria_pinder_ids['train'])\n",
    "eubacteria_train_index = train_index[mask]\n",
    "eubacteria_pinder_ids['train'] = reduce_train_redundancy(eubacteria_pinder_ids['train'], eubacteria_train_index, max_per_cluster=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "67917238-e0d4-40e5-84d5-25036fa55f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3144 680 645\n"
     ]
    }
   ],
   "source": [
    "# Number of positive elements in each split after redundancy removal\n",
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e030bdd6-7dfd-4415-aaf9-fabd2dc1606d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\tKL(1|0)\tKL(0|1)\n",
      "train\t0.436\t0.266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"split\\tKL(1|0)\\tKL(0|1)\")\n",
    "\n",
    "for split in [\"train\"]:\n",
    "    positives[split] = get_positives_df(split, eubacteria_pinder_ids, eubacteria_pinder_phyla)\n",
    "    \n",
    "    negatives[split] = make_negative_pairs(positives[split], \n",
    "        positives[\"train\"], eubacteria_annotated_interactions, \n",
    "        eubacteria_GO, tax_level=TAX_LEVEL\n",
    "    )\n",
    "\n",
    "    pos_seq[split] = get_sequences(positives[split][:n_samples])\n",
    "    neg_seq[split] = get_sequences(negatives[split][:n_samples])\n",
    "    \n",
    "    pos_embs_R[split], pos_embs_L[split] = generate_embeddings(\n",
    "        positives[split][:n_samples], pos_seq[split], model, tokenizer, \n",
    "        num_samples=len(positives[split][:n_samples]), emb_dim=1280\n",
    "    )\n",
    "    neg_embs_R[split], neg_embs_L[split] = generate_embeddings(\n",
    "        negatives[split][:n_samples], neg_seq[split], model, tokenizer, \n",
    "        num_samples=len(negatives[split][:n_samples]), emb_dim=1280\n",
    "    )\n",
    "\n",
    "    pos_dists[split] = torch.cdist(\n",
    "        pos_embs_R[split], \n",
    "        pos_embs_L[split]\n",
    "    ).diag()\n",
    "    neg_dists[split] = torch.cdist(\n",
    "        neg_embs_R[split], \n",
    "        neg_embs_L[split]\n",
    "    ).diag()\n",
    "\n",
    "    # Plot distances distribution\n",
    "    hist_pos[split], hist_neg[split] = plot_distributions(\n",
    "        pos_dists[split], neg_dists[split], \n",
    "        f\"Distance distribution of {split} subset\", split\n",
    "    )\n",
    "\n",
    "    # Compute KL divergence\n",
    "    kl10, kl01 = compute_KL_divergence(hist_pos[split], hist_neg[split])\n",
    "    print(f\"{split}\\t{kl10:.3f}\\t{kl01:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "892cfa16-7a58-49ad-a265-489b1cc52a38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3144 2347 680 458 645 434\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    len(positives['train']),\n",
    "    len(negatives['train']),\n",
    "    len(positives['val']),\n",
    "    len(negatives['val']),\n",
    "    len(positives['test']),\n",
    "    len(negatives['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "30f2a12b-c7b2-48fd-afc2-cd614ed86cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 18 213 433 659 297 648 444 616 597 527 241 606 635 180 296 625  96 365\n",
      " 649 599 191 665 614 270 181 463 196 210 353 243 596 190 205 277 404 161\n",
      "  47  22 385 163 257 410 466 263 344 218 159 582 398 447 269 185 423 154\n",
      " 402 366  11 650  37  31 577 623 543 545 294  72 460 390 299 580 179 140\n",
      " 461 245 515 449 495  81 207 198 501 537 448 330   5 256 620 595 193  36\n",
      " 500 612 226 470 605 286 530 146 323 346  95 314 477 565 622 290  62  10\n",
      "  78 453   3 522 138 662 216  70 348 264  19 571 232 627 452 260 332 498\n",
      " 283 359  65 415   4   1 194 153 472 675 151 437 227  79 252 125 491 368\n",
      " 187 338 212 473  69 164 197  74   0 117 672 652 536 668 488 305 588 578\n",
      " 322 504  64   7 115 417 476  58 538 485 244 133 409 318 435 249 238 601\n",
      "  12 478 312 670 570 329 204 174 618 660 274 400 394 615 217 467 502  39\n",
      " 547 255  26 237 426 643 137 246  40 443 200  54 280 586 462 262 108 544\n",
      " 206 288 425 284 581 275 251 496 158 663 358 298 655 356 469 360 279  16\n",
      " 503 628 195 519 136 590 339 507 439 380 533 506 273 603 440 405 118 316\n",
      " 567 325 408  55 110 579  57 315  61  15 486 564 468  33 135 377 576 637\n",
      " 106 593 526 397  29   6 113 166  38 414 563 540  35 337 160  93 289 375\n",
      " 357 276 591 152 574 511 585 177 172 293  75   2 327 656 645 442 225 430\n",
      " 520  82 266 607 374 445 235 240 236 499  51 139 557 403 186 114 388 487\n",
      " 474 324 609 509 514 550 678 192 624  17  87 630 673 490 633 535 542 629\n",
      " 419 572   9 147 230 100 116 122 666 371 124 539  99 128 679 219  24 119\n",
      " 224 677 306  71 121 308 104 676 395 130 621  42 598  34 364  88 141 362\n",
      "  21  77 592 211 250 233 657 639  68 382 489 483 109 369 168 208 310  98\n",
      " 281 484 214 268  83 546 432 234 617 102 162 157 653 479 376  94 350 534\n",
      " 413 456 464 341  85 604 248 529 549 105 524 367 386 434 171 334 636 120\n",
      " 370 352 183 209 292 282 552 131 343 254 150 587 619 267 313 613 556 321\n",
      " 271 247 420 600 428 253 307 412]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([6.91099194e-03, 1.90052278e-04, 2.24607238e-04, 1.38219839e-04,\n",
       "        1.72774798e-04, 1.20942359e-04, 1.03664879e-04, 0.00000000e+00,\n",
       "        0.00000000e+00, 5.18324395e-05]),\n",
       " array([5.00031228e-05, 8.12550746e-04, 1.11256948e-03, 1.12507026e-03,\n",
       "        1.26257885e-03, 7.50046842e-04, 4.12525763e-04, 7.50046842e-05,\n",
       "        7.50046842e-05, 5.00031228e-05]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Downsample positives\n",
    "\n",
    "split = 'val'\n",
    "\n",
    "# Convert tensors to numpy if needed\n",
    "neg_dists_np = neg_dists[split].cpu().numpy()\n",
    "pos_dists_np = pos_dists[split].cpu().numpy()\n",
    "\n",
    "# Create histogram of negative distances\n",
    "neg_hist, bin_edges = np.histogram(neg_dists_np, bins=50, density=True)\n",
    "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
    "\n",
    "# Map each positive distance to its corresponding histogram bin\n",
    "bin_indices = np.digitize(pos_dists_np, bins=bin_edges) - 1\n",
    "bin_indices = np.clip(bin_indices, 0, len(neg_hist) - 1)\n",
    "\n",
    "# Use negative histogram values as sampling probabilities\n",
    "sampling_probs = neg_hist[bin_indices]\n",
    "sampling_probs /= sampling_probs.sum()  # normalize\n",
    "\n",
    "from numpy.random import choice\n",
    "\n",
    "n_to_sample = len(neg_dists_np)  # Match number of negatives, or any number you want\n",
    "\n",
    "sampled_pos_indices = choice(np.arange(len(pos_dists_np)), size=n_to_sample, replace=False, p=sampling_probs)\n",
    "\n",
    "# Update positive data structures with downsampled versions\n",
    "positives_ds = {}\n",
    "pos_seq_ds = {}\n",
    "pos_embs_R_ds = {}\n",
    "pos_embs_L_ds = {}\n",
    "pos_dists_ds = {}\n",
    "\n",
    "positives_ds[split] = positives[split].iloc[sampled_pos_indices].reset_index(drop=True)\n",
    "\n",
    "# Filter embeddings\n",
    "pos_embs_R_ds[split] = pos_embs_R[split][sampled_pos_indices]\n",
    "pos_embs_L_ds[split] = pos_embs_L[split][sampled_pos_indices]\n",
    "pos_dists_ds[split] = pos_dists[split][sampled_pos_indices]\n",
    "\n",
    "plot_distributions(\n",
    "    pos_dists_ds[split], neg_dists[split], \n",
    "    f\"Downsampled Distance Distribution - {split}\", f\"{split}_downsampled\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1ca0c57b-ee13-4952-87b9-bb0d5588273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tClass\tNumber of instances\n",
      "train\t0\t4617\n",
      "train\t1\t4617\n",
      "val\t0\t344\n",
      "val\t1\t344\n",
      "test\t0\t332\n",
      "test\t1\t332\n"
     ]
    }
   ],
   "source": [
    "# Save IDs and phyla\n",
    "\n",
    "print(f\"Split\\tClass\\tNumber of instances\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_positives = []\n",
    "    for id in eubacteria_pinder_ids[split]:\n",
    "        record = {\n",
    "            \"pinder_id\": id,\n",
    "            \"pinder_id_R\": id.split(\"--\")[0], \n",
    "            \"pinder_id_L\": id.split(\"--\")[1], \n",
    "            f\"{TAX_LEVEL}_R\": eubacteria_pinder_phyla[id][0], \n",
    "            f\"{TAX_LEVEL}_L\": eubacteria_pinder_phyla[id][1]}\n",
    "        split_positives.append(record)\n",
    "    \n",
    "    split_positives = pd.DataFrame.from_dict(split_positives)\n",
    "    split_negatives = make_negative_pairs(split_positives, eubacteria_annotated_interactions, eubacteria_GO, tax_level=TAX_LEVEL)\n",
    "    split_positives = split_positives[[\"pinder_id_R\", \"pinder_id_L\", f\"{TAX_LEVEL}_R\", f\"{TAX_LEVEL}_L\"]]\n",
    "\n",
    "    split_positives = rename_columns(split_positives, tax_level=TAX_LEVEL)\n",
    "    split_negatives = rename_columns(split_negatives, to_drop=\"pinder_id\", tax_level=TAX_LEVEL)\n",
    "    \n",
    "    # Balancing the set\n",
    "    split_positives = split_positives.sample(n=len(split_negatives), random_state=SEED)\n",
    "\n",
    "    # Labelling the set\n",
    "    split_negatives['label'] = [0]*len(split_negatives)\n",
    "    split_positives['label'] = [1]*len(split_positives)\n",
    "    \n",
    "    # Summary of the split\n",
    "    print(f\"{split}\\t0\\t{len(split_negatives)}\\n{split}\\t1\\t{len(split_positives)}\")\n",
    "\n",
    "    # Merging the classes\n",
    "    split_all = pd.concat([split_positives, split_negatives])\n",
    "    \n",
    "    # Shuffling\n",
    "    split_all = split_all.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    # Saving\n",
    "    split_all.to_csv(f\"{SPLITS_DIR}/{split}.txt\", sep=\"\\t\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4ecb63c4-1e90-4162-afd4-b8d6a2f1882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\", \"test\"]: make_split_fasta(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e9984fba-c401-435b-a835-36a64fa2e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $SPLITS_DIR/train.fasta $SPLITS_DIR/val.fasta $SPLITS_DIR/test.fasta > $SPLITS_DIR/sequences.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc4630-47e4-43dd-8fff-fc3ad64f38c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glam",
   "language": "python",
   "name": "glam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
