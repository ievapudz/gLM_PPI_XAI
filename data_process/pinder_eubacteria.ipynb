{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8e372a53-282c-4c38-b24b-e96982434ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from pinder.core import get_index, PinderSystem\n",
    "from pinder.core import PinderLoader\n",
    "from pinder.core.loader import filters\n",
    "from pinder.core.loader import structure\n",
    "from pinder.core.index.system import PinderSystem\n",
    "from pinder.core import get_pinder_location\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "PINDER_BASE_DIR=\"/scicore/home/schwede/durair0000/.local/share/\"\n",
    "UNIPROT_IDS_PATH=\"/scicore/home/schwede/pudziu0000/projects/gLM/data/PINDER/uniprot_ids_1024_512.lst\"\n",
    "UNIPROT_LINEAGES_PATH=\"/scicore/home/schwede/pudziu0000/projects/gLM/data/PINDER/uniprot_lineages_annotations_1024_512.tsv\"\n",
    "SPLITS_DIR=\"/scicore/home/schwede/pudziu0000/projects/gLM/data/PINDER/eubacteria_5_1024_512/\"\n",
    "SEED=1024\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def select_ids(max_length=1024, max_length_per_monomer=400):\n",
    "    base_filters = [\n",
    "        filters.FilterByMissingHolo(),\n",
    "        filters.FilterSubByContacts(min_contacts=5, radius=10.0, calpha_only=True),\n",
    "        filters.FilterDetachedHolo(radius=12, max_components=2),\n",
    "    ]\n",
    "    sub_filters = [\n",
    "        filters.FilterSubByAtomTypes(min_atom_types=4),\n",
    "        filters.FilterByHoloOverlap(min_overlap=5),\n",
    "        filters.FilterByHoloSeqIdentity(min_sequence_identity=0.8),\n",
    "        filters.FilterSubRmsds(rmsd_cutoff=7.5),\n",
    "        filters.FilterDetachedSub(radius=12, max_components=2),\n",
    "    ]\n",
    "    split_pinder_ids = {}\n",
    "    split_uniprot_ids = {}\n",
    "    train_index = None\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        loader = PinderLoader(\n",
    "            split=split,\n",
    "            monomer_priority=\"holo\",\n",
    "            base_filters=base_filters,\n",
    "            sub_filters=sub_filters,\n",
    "        )\n",
    "            \n",
    "        index = loader.index.merge(loader.metadata, on=\"id\", how=\"left\")\n",
    "        index[\"length\"] = index[\"length1\"] + index[\"length2\"]\n",
    "        index[\"resolution\"] = index[\"resolution\"].astype(\"float32\")\n",
    "        index = index[\n",
    "            (index[\"length\"] + 2 <= max_length)\n",
    "            & (index[\"length1\"] <= max_length_per_monomer)\n",
    "            & (index[\"length2\"] <= max_length_per_monomer)\n",
    "            & (index[\"label\"] == \"BIO\")\n",
    "        ]\n",
    "        if(split == \"train\"): train_index = index\n",
    "        pinder_ids = list(index[\"id\"])\n",
    "        uniprot_ids = set(list(index[\"uniprot_R\"]) + list(index[\"uniprot_L\"]))\n",
    "        split_pinder_ids[split] = pinder_ids\n",
    "        split_uniprot_ids[split] = uniprot_ids\n",
    "    return split_pinder_ids, split_uniprot_ids, train_index\n",
    "\n",
    "def reduce_train_redundancy(ids, index, max_per_cluster=5):\n",
    "    # ids - list of ids of the training set\n",
    "    # index - index of the training set\n",
    "    ids = list(\n",
    "        set(\n",
    "            index.sort_values(\n",
    "                [\"resolution\", \"id\"],\n",
    "                ascending=[True, True],\n",
    "            )\n",
    "            .groupby(\"cluster_id\", observed=False)\n",
    "            .head(max_per_cluster)[\"id\"]\n",
    "        )\n",
    "    )\n",
    "    return ids\n",
    "\n",
    "def make_random_pairs(L, pdb_L, chain_uniprot_L, uniprot_L, \n",
    "                      pdb_R, uniprot_R,\n",
    "                      interacting_partners, GO, max_num_attempts):\n",
    "    invalidity_flags = []\n",
    "    # Shuffle until no same PDB pairs\n",
    "    for attempt in range(max_num_attempts):  # Limit reattempts\n",
    "        L_info = list(zip(L, pdb_L, chain_uniprot_L, uniprot_L))\n",
    "        random.Random(SEED).shuffle(L_info)\n",
    "        L, pdb_L, chain_uniprot_L, uniprot_L = zip(*L_info)\n",
    "        \n",
    "        # If UniProt IDs of the new pair are known to contain interacting sequences, retry\n",
    "        if any(uL in interacting_partners[uR] for uR, uL in zip(uniprot_R, uniprot_L)):\n",
    "            if(attempt != max_num_attempts-1):\n",
    "                continue\n",
    "            else:\n",
    "                invalidity_flags.append(\"interacting\")\n",
    "                \n",
    "        # If UniProt IDs of the new pair have overlapping Molecular Function GOs, retry\n",
    "        if any(bool(set(GO[uR]) & set(GO[uL])) for uR, uL in zip(uniprot_R, uniprot_L)):\n",
    "            if(attempt != max_num_attempts-1):\n",
    "                continue\n",
    "            else:\n",
    "                invalidity_flags.append(\"GO\")\n",
    "\n",
    "        # If PDB IDs of all pairs are non-matching, call it a success\n",
    "        if all(pR != pL for pR, pL in zip(pdb_R, pdb_L)): break\n",
    "\n",
    "    return L, pdb_L, chain_uniprot_L, uniprot_L, invalidity_flags\n",
    "\n",
    "\n",
    "def remove_false_negatives(R, L, uniprot_R, uniprot_L, interacting_partners, \n",
    "                           GO, invalidity_flags, orig_pinder_ids):\n",
    "    # If there are invalid pairings, execute removals\n",
    "    if(invalidity_flags):\n",
    "        to_remove = set()\n",
    "        for i, (uR, uL) in enumerate(zip(uniprot_R, uniprot_L)):\n",
    "            if \"interacting\" in invalidity_flags and uL in interacting_partners[uR]: to_remove.add(i)\n",
    "            if \"GO\" in invalidity_flags and bool(set(GO[uR]) & set(GO[uL])): to_remove.add(i)\n",
    "        \n",
    "        valid_indices = set(range(len(R)))-to_remove\n",
    "        R = [R[i] for i in valid_indices]\n",
    "        L = [L[i] for i in valid_indices]\n",
    "    \n",
    "    # Forming new pinder_ids\n",
    "    paired = zip(R, L)\n",
    "    paired = [\"--\".join(pair) for pair in paired]\n",
    "    \n",
    "    # If new pinder_id is present in the original set, remove that record\n",
    "    paired = [item for item in paired if item not in orig_pinder_ids]\n",
    "\n",
    "    return paired\n",
    "\n",
    "def make_negative_pairs(df, interacting_partners, GO, seed=SEED, max_num_attempts=100):    \n",
    "    # Separate ids\n",
    "    df[['R', 'L']] = df['pinder_id'].str.split('--', expand=True)\n",
    "    df[['pdb_R', 'chain_uniprot_R']] = df['R'].str.split('__', expand=True)\n",
    "    df[['chain_R', 'uniprot_R']] = df['chain_uniprot_R'].str.split('_', expand=True)\n",
    "    df[['pdb_L', 'chain_uniprot_L']] = df['L'].str.split('__', expand=True)\n",
    "    df[['chain_L', 'uniprot_L']] = df['chain_uniprot_L'].str.split('_', expand=True)\n",
    "    \n",
    "    # Group by phylum\n",
    "    negative_ids = []\n",
    "    negative_phyla = []\n",
    "    orig_pinder_ids = set(df['pinder_id'])\n",
    "    \n",
    "    for phylum, group in df.groupby('phylum_R'):\n",
    "        \n",
    "        R = group['R'].tolist()\n",
    "        L = group['L'].tolist()\n",
    "        pdb_R = group['pdb_R'].tolist()\n",
    "        pdb_L = group['pdb_L'].tolist()\n",
    "        chain_uniprot_L = group['chain_uniprot_L'].tolist()\n",
    "        uniprot_R = group['uniprot_R'].tolist()\n",
    "        uniprot_L = group['uniprot_L'].tolist()\n",
    "\n",
    "        L, pdb_L, chain_uniprot_L, uniprot_L, invalidity_flags = make_random_pairs(\n",
    "            L, pdb_L, chain_uniprot_L, uniprot_L, pdb_R, uniprot_R, \n",
    "            interacting_partners, GO, max_num_attempts\n",
    "        )\n",
    "        \n",
    "        # If there are invalid pairings, execute removals and get pairs\n",
    "        paired = remove_false_negatives(R, L, uniprot_R, uniprot_L, \n",
    "            interacting_partners, GO, invalidity_flags, orig_pinder_ids\n",
    "        )\n",
    "\n",
    "        negative_ids.extend(paired)\n",
    "        negative_phyla.extend([phylum]*len(paired))\n",
    "\n",
    "    negative_records = list(zip(negative_ids, negative_phyla, negative_phyla))\n",
    "    negative_df = pd.DataFrame(\n",
    "        negative_records, \n",
    "        columns=['pinder_id', 'phylum_R', 'phylum_L']\n",
    "    ).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    negative_df[['pinder_id_R', 'pinder_id_L']] = negative_df['pinder_id'].str.split('--', expand=True)\n",
    "    \n",
    "    return negative_df\n",
    "\n",
    "def rename_columns(df, to_drop=None):\n",
    "    df.rename(columns={\n",
    "        \"pinder_id_R\": \"protein1\",\n",
    "        \"pinder_id_L\": \"protein2\",\n",
    "        \"phylum_R\": \"phylum1\",\n",
    "        \"phylum_L\": \"phylum2\"\n",
    "    }, inplace=True)\n",
    "    if(to_drop): df.drop([to_drop], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def make_split_fasta(split):\n",
    "    df = pd.read_csv(f\"{SPLITS_DIR}/{split}.txt\", sep=\"\\t\")\n",
    "\n",
    "    fasta = []\n",
    "    # Reducing duplicates\n",
    "    ids = {\"R\": set(), \"L\": set()}\n",
    "    for i, j in df.iterrows():\n",
    "        idR = df[\"protein1\"][i]\n",
    "        idL = df[\"protein2\"][i]\n",
    "        ids[\"R\"].add(idR)\n",
    "        ids[\"L\"].add(idL)\n",
    "\n",
    "    for k in ids:\n",
    "        for id_ in ids[k]:\n",
    "            struct = structure.Structure(f\"{PINDER_BASE_DIR}/pinder/2024-02/pdbs/{id_}-{k}.pdb\", pinder_id=id_)\n",
    "            fasta.append(SeqRecord(Seq(struct.sequence), id=id_, description=\"\"))\n",
    "    \n",
    "    with open(f\"{SPLITS_DIR}/{split}.fasta\", \"w\") as output_handle:\n",
    "        SeqIO.write(fasta, output_handle, \"fasta-2line\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de970895-ea39-4f2b-b294-80003d7d5ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/scicore/home/schwede/durair0000/.local/share/pinder/2024-02')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PINDER_BASE_DIR\"] = PINDER_BASE_DIR\n",
    "get_pinder_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f007fc0d-da6f-4e71-bac5-7b8cface420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pinder_ids, uniprot_ids, train_index = select_ids(max_length_per_monomer=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f82601cf-96a0-48b5-a2c0-badaccf3e11f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "675729 1799 1767\n",
      "20907 1937 1906\n"
     ]
    }
   ],
   "source": [
    "# Checking number of elements in each split\n",
    "print(len(pinder_ids['train']),\n",
    "      len(pinder_ids['val']),\n",
    "      len(pinder_ids['test'])\n",
    ")\n",
    "\n",
    "print(len(uniprot_ids['train']),\n",
    "      len(uniprot_ids['val']),\n",
    "      len(uniprot_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7df3af2a-b115-4ec8-8950-247b1a3de70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting that that both defined UniProt IDs\n",
    "pinder_ids['train'] = [row for row in pinder_ids['train'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "pinder_ids['val'] = [row for row in pinder_ids['val'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "pinder_ids['test'] = [row for row in pinder_ids['test'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "\n",
    "uniprot_ids['train'].remove(\"UNDEFINED\")\n",
    "uniprot_ids['val'].remove(\"UNDEFINED\")\n",
    "uniprot_ids['test'].remove(\"UNDEFINED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ebdca60-6e15-4274-ae87-e8fdf22ef01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "617197 1748 1688\n",
      "20906 1936 1905\n"
     ]
    }
   ],
   "source": [
    "# Checking number of elements in each split after removal of \"UNDEFINED\"\n",
    "print(len(pinder_ids['train']),\n",
    "      len(pinder_ids['val']),\n",
    "      len(pinder_ids['test'])\n",
    ")\n",
    "\n",
    "print(len(uniprot_ids['train']),\n",
    "      len(uniprot_ids['val']),\n",
    "      len(uniprot_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "073c00a5-62d8-4353-978b-2055d57b4dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save UniProt IDs to a TXT file for \"ID mapping\"\n",
    "with open(UNIPROT_IDS_PATH, 'w') as f:\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for line in uniprot_ids[split]:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b0b7f998-e9ce-4018-9eed-9e1ebd5bd706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually run the ID mapping on the UniProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67c3212f-9a63-4823-bfe3-2215349b46c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine, which UniProt IDs are belonging to eubacteria with labelled phylum\n",
    "lineages = pd.read_csv(UNIPROT_LINEAGES_PATH, sep='\\t')\n",
    "\n",
    "eubacteria_phyla = {}\n",
    "eubacteria_annotated_interactions = {}\n",
    "eubacteria_GO = {}\n",
    "\n",
    "for index, row in lineages.iterrows():\n",
    "    if isinstance(row['Taxonomic lineage (Ids)'], str) and \"2 (superkingdom)\" in row['Taxonomic lineage (Ids)']: \n",
    "        match = re.search(r'(\\d+) \\(phylum\\)', row['Taxonomic lineage (Ids)'])\n",
    "        if match:\n",
    "            phylum_tax_id = match.group(1)\n",
    "            eubacteria_phyla[row['From']] = phylum_tax_id\n",
    "            \n",
    "            # Retrieving UniProt IDs of the known interacting partners\n",
    "            if(not pd.isna(row['Interacts with'])):\n",
    "                eubacteria_annotated_interactions[row['From']] = [item.split('-')[0] for item in row['Interacts with'].split('; ')]\n",
    "            else:\n",
    "                eubacteria_annotated_interactions[row['From']] = []\n",
    "            \n",
    "            # Retrieving GO \n",
    "            if(not pd.isna(row['Gene Ontology (molecular function)'])):\n",
    "                eubacteria_GO[row['From']] = re.findall(r\"GO:\\d+\", row['Gene Ontology (molecular function)'])\n",
    "            else:\n",
    "                eubacteria_GO[row['From']] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "284544b4-bae4-48a3-8087-f21504d0745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine, which PINDER IDs contain both bacterial proteins\n",
    "eubacteria_pinder_ids = {'train': [], 'val': [], 'test': []}\n",
    "eubacteria_pinder_phyla = {}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for id in pinder_ids[split]:\n",
    "        uniprot1 = id.split(\"--\")[0].split(\"_\")[-1]\n",
    "        uniprot2 = id.split(\"--\")[1].split(\"_\")[-1]\n",
    "        if(uniprot1 in eubacteria_phyla.keys() and uniprot2 in eubacteria_phyla.keys()):\n",
    "            eubacteria_pinder_ids[split].append(id)\n",
    "            eubacteria_pinder_phyla[id] = [eubacteria_phyla[uniprot1], eubacteria_phyla[uniprot2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bf36719-20e2-4059-941f-e34aa313d12b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86221 906 885\n"
     ]
    }
   ],
   "source": [
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c897770-537a-442b-96d9-7c13bfd4984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundancy from the training set\n",
    "mask = train_index['id'].isin(eubacteria_pinder_ids['train'])\n",
    "eubacteria_train_index = train_index[mask]\n",
    "eubacteria_pinder_ids['train'] = reduce_train_redundancy(eubacteria_pinder_ids['train'], eubacteria_train_index, max_per_cluster=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "259f3538-e7e8-4fa2-907b-aaf46d39d17c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8283 906 885\n"
     ]
    }
   ],
   "source": [
    "# Number of elements in each split after redundancy removal\n",
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1ca0c57b-ee13-4952-87b9-bb0d5588273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tClass\tNumber of instances\n",
      "train\t0\t7533\n",
      "train\t1\t7533\n",
      "val\t0\t792\n",
      "val\t1\t792\n",
      "test\t0\t790\n",
      "test\t1\t790\n"
     ]
    }
   ],
   "source": [
    "# Save IDs and phyla\n",
    "\n",
    "print(f\"Split\\tClass\\tNumber of instances\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    split_positives = []\n",
    "    for id in eubacteria_pinder_ids[split]:\n",
    "        record = {\n",
    "            \"pinder_id\": id,\n",
    "            \"pinder_id_R\": id.split(\"--\")[0], \n",
    "            \"pinder_id_L\": id.split(\"--\")[1], \n",
    "            \"phylum_R\": eubacteria_pinder_phyla[id][0], \n",
    "            \"phylum_L\": eubacteria_pinder_phyla[id][1]}\n",
    "        split_positives.append(record)\n",
    "\n",
    "    split_positives = pd.DataFrame.from_dict(split_positives)\n",
    "    split_negatives = make_negative_pairs(split_positives, eubacteria_annotated_interactions, eubacteria_GO)\n",
    "    split_positives = split_positives[[\"pinder_id_R\", \"pinder_id_L\", \"phylum_R\", \"phylum_L\"]]\n",
    "\n",
    "    split_positives = rename_columns(split_positives)\n",
    "    split_negatives = rename_columns(split_negatives, to_drop=\"pinder_id\")\n",
    "    \n",
    "    # Balancing the set\n",
    "    split_positives = split_positives.sample(n=len(split_negatives), random_state=SEED)\n",
    "\n",
    "    # Labelling the set\n",
    "    split_negatives['label'] = [0]*len(split_negatives)\n",
    "    split_positives['label'] = [1]*len(split_positives)\n",
    "\n",
    "    # Summary of the split\n",
    "    print(f\"{split}\\t0\\t{len(split_negatives)}\\n{split}\\t1\\t{len(split_positives)}\")\n",
    "\n",
    "    # Merging the classes\n",
    "    split_all = pd.concat([split_positives, split_negatives])\n",
    "    \n",
    "    # Shuffling\n",
    "    split_all = split_all.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    # Saving\n",
    "    split_all.to_csv(f\"{SPLITS_DIR}/{split}.txt\", sep=\"\\t\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4ecb63c4-1e90-4162-afd4-b8d6a2f1882b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"train\", \"val\", \"test\"]: make_split_fasta(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e9984fba-c401-435b-a835-36a64fa2e921",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat $SPLITS_DIR/train.fasta $SPLITS_DIR/val.fasta $SPLITS_DIR/test.fasta > $SPLITS_DIR/sequences.fasta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glam",
   "language": "python",
   "name": "glam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
