{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29ddf00c-0ab9-44b7-ba97-bbe4e0b9e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pinder.core import get_index, PinderSystem\n",
    "from pinder.core import PinderLoader\n",
    "from pinder.core.loader import filters\n",
    "from pinder.core.loader import structure\n",
    "from pinder.core.index.system import PinderSystem\n",
    "from pinder.core import get_pinder_location\n",
    "import re\n",
    "import pandas as pd\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.special import rel_entr\n",
    "\n",
    "TAX_LEVEL=\"species\"\n",
    "\n",
    "HOME=\"/scicore/home/schwede/pudziu0000/\"\n",
    "PINDER_BASE_DIR=\"/scicore/home/schwede/durair0000/.local/share/\"\n",
    "UNIPROT_IDS_PATH=f\"{HOME}/projects/gLM/data/PINDER/uniprot_ids_1024_512.lst\"\n",
    "UNIPROT_LINEAGES_PATH=f\"{HOME}/projects/gLM/data/PINDER/uniprot_lineages_annotations_1024_512.tsv\"\n",
    "SPLITS_DIR=f\"{HOME}/projects/gLM/data/PINDER/eubacteria_5_1024_512_{TAX_LEVEL}_harder_test/\"\n",
    "SEED=1024\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = AutoModel.from_pretrained(f\"{HOME}/projects/gLM/gLM2_650M\", torch_dtype=torch.bfloat16, trust_remote_code=True).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"{HOME}/projects/gLM/gLM2_650M\", trust_remote_code=True)\n",
    "\n",
    "random.seed(SEED)\n",
    "\n",
    "def select_ids(max_length=1024, max_length_per_monomer=400):\n",
    "    base_filters = [\n",
    "        filters.FilterByMissingHolo(),\n",
    "        filters.FilterSubByContacts(min_contacts=5, radius=10.0, calpha_only=True),\n",
    "        filters.FilterDetachedHolo(radius=12, max_components=2),\n",
    "    ]\n",
    "    sub_filters = [\n",
    "        filters.FilterSubByAtomTypes(min_atom_types=4),\n",
    "        filters.FilterByHoloOverlap(min_overlap=5),\n",
    "        filters.FilterByHoloSeqIdentity(min_sequence_identity=0.8),\n",
    "        filters.FilterSubRmsds(rmsd_cutoff=7.5),\n",
    "        filters.FilterDetachedSub(radius=12, max_components=2),\n",
    "    ]\n",
    "    split_pinder_ids = {}\n",
    "    split_uniprot_ids = {}\n",
    "    train_index = None\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        loader = PinderLoader(\n",
    "            split=split,\n",
    "            monomer_priority=\"holo\",\n",
    "            base_filters=base_filters,\n",
    "            sub_filters=sub_filters,\n",
    "        )\n",
    "            \n",
    "        index = loader.index.merge(loader.metadata, on=\"id\", how=\"left\")\n",
    "        index[\"length\"] = index[\"length1\"] + index[\"length2\"]\n",
    "        index[\"resolution\"] = index[\"resolution\"].astype(\"float32\")\n",
    "        index = index[\n",
    "            (index[\"length\"] + 2 <= max_length)\n",
    "            & (index[\"length1\"] <= max_length_per_monomer)\n",
    "            & (index[\"length2\"] <= max_length_per_monomer)\n",
    "            & (index[\"label\"] == \"BIO\")\n",
    "        ]\n",
    "        if(split == \"train\"): \n",
    "            index = index[\n",
    "                (index[\"length\"] + 2 <= max_length)\n",
    "                & (index[\"length1\"] <= max_length_per_monomer)\n",
    "                & (index[\"length2\"] <= max_length_per_monomer)\n",
    "                & (index[\"label\"] == \"BIO\")\n",
    "                & (index[\"cluster_id_R\"] != index[\"cluster_id_L\"])  # Filter to keep only heterodimers in training\n",
    "            ]\n",
    "            train_index = index\n",
    "        pinder_ids = list(index[\"id\"])\n",
    "        uniprot_ids = set(list(index[\"uniprot_R\"]) + list(index[\"uniprot_L\"]))\n",
    "        split_pinder_ids[split] = pinder_ids\n",
    "        split_uniprot_ids[split] = uniprot_ids\n",
    "    return split_pinder_ids, split_uniprot_ids, train_index\n",
    "\n",
    "def reduce_train_redundancy(ids, index, max_per_cluster=5):\n",
    "    # ids - list of ids of the training set\n",
    "    # index - index of the training set\n",
    "    ids = list(\n",
    "        set(\n",
    "            index.sort_values(\n",
    "                [\"resolution\", \"id\"],\n",
    "                ascending=[True, True],\n",
    "            )\n",
    "            .groupby(\"cluster_id\", observed=False)\n",
    "            .head(max_per_cluster)[\"id\"]\n",
    "        )\n",
    "    )\n",
    "    return ids\n",
    "\n",
    "def get_positives_df(split, pinder_ids, pinder_tax_annot):\n",
    "    positives = []\n",
    "    for id_ in eubacteria_pinder_ids[split]:\n",
    "        record = {\n",
    "            \"pinder_id\": id_,\n",
    "            \"pinder_id_R\": id_.split(\"--\")[0], \n",
    "            \"pinder_id_L\": id_.split(\"--\")[1], \n",
    "            f\"{TAX_LEVEL}_R\": pinder_tax_annot[id_][0],\n",
    "            f\"{TAX_LEVEL}_L\": pinder_tax_annot[id_][1]}\n",
    "        positives.append(record)\n",
    "    return pd.DataFrame.from_dict(positives)\n",
    "\n",
    "def make_random_pairs(L, pdb_L, chain_uniprot_L, uniprot_L, \n",
    "                      pdb_R, uniprot_R,\n",
    "                      interacting_partners, GO, max_num_attempts):\n",
    "    invalidity_flags = []\n",
    "    # Shuffle until no same PDB pairs\n",
    "    for attempt in range(max_num_attempts):  # Limit reattempts\n",
    "        L_info = list(zip(L, pdb_L, chain_uniprot_L, uniprot_L))\n",
    "        if(len(L_info) == 0): return (None, None, None, None, None)\n",
    "        random.Random(SEED).shuffle(L_info)\n",
    "        L, pdb_L, chain_uniprot_L, uniprot_L = zip(*L_info)\n",
    "        \n",
    "        # If UniProt IDs of the new pair are known to contain interacting sequences, retry\n",
    "        if any(uL in interacting_partners[uR] for uR, uL in zip(uniprot_R, uniprot_L)):\n",
    "            if(attempt != max_num_attempts-1):\n",
    "                continue\n",
    "            else:\n",
    "                invalidity_flags.append(\"interacting\")\n",
    "                \n",
    "        # If UniProt IDs of the new pair have overlapping Molecular Function GOs, retry\n",
    "        if any(bool(set(GO[uR]) & set(GO[uL])) for uR, uL in zip(uniprot_R, uniprot_L)):\n",
    "            if(attempt != max_num_attempts-1):\n",
    "                continue\n",
    "            else:\n",
    "                invalidity_flags.append(\"GO\")\n",
    "\n",
    "        # If PDB IDs of all pairs are non-matching, call it a success\n",
    "        if all(pR != pL for pR, pL in zip(pdb_R, pdb_L)): break\n",
    "\n",
    "    return L, pdb_L, chain_uniprot_L, uniprot_L, invalidity_flags\n",
    "\n",
    "\n",
    "def remove_false_negatives(R, L, uniprot_R, uniprot_L, interacting_partners, \n",
    "                           GO, invalidity_flags, orig_pinder_ids):\n",
    "    # If there are invalid pairings, execute removals\n",
    "    if(invalidity_flags):\n",
    "        to_remove = set()\n",
    "        for i, (uR, uL) in enumerate(zip(uniprot_R, uniprot_L)):\n",
    "            if \"interacting\" in invalidity_flags and uL in interacting_partners[uR]: to_remove.add(i)\n",
    "            if \"GO\" in invalidity_flags and bool(set(GO[uR]) & set(GO[uL])): to_remove.add(i)\n",
    "        \n",
    "        valid_indices_R = set(range(len(R)))-to_remove\n",
    "        valid_indices_L = set(range(len(L)))-to_remove\n",
    "        R = [R[i] for i in valid_indices_R]\n",
    "        L = [L[i] for i in valid_indices_L]\n",
    "\n",
    "    # Forming new pinder_ids\n",
    "    paired = zip(R, L)\n",
    "    paired = [\"--\".join(pair) for pair in paired]\n",
    "    \n",
    "    # If new pinder_id is present in the original set, remove that record\n",
    "    paired = [item for item in paired if item not in orig_pinder_ids]\n",
    "\n",
    "    return paired\n",
    "\n",
    "def make_negative_pairs(dfR, dfL, interacting_partners, GO, seed=SEED, max_num_attempts=5, tax_level=\"phylum\"):    \n",
    "    # Separate ids\n",
    "    \n",
    "    dfR[['R', 'L']] = dfR['pinder_id'].str.split('--', expand=True)\n",
    "    dfR[['pdb_R', 'chain_uniprot_R']] = dfR['R'].str.split('__', expand=True)\n",
    "    dfR[['chain_R', 'uniprot_R']] = dfR['chain_uniprot_R'].str.split('_', expand=True)\n",
    "    dfR[['pdb_L', 'chain_uniprot_L']] = dfR['L'].str.split('__', expand=True)\n",
    "    dfR[['chain_L', 'uniprot_L']] = dfR['chain_uniprot_L'].str.split('_', expand=True)\n",
    "\n",
    "    dfL[['R', 'L']] = dfL['pinder_id'].str.split('--', expand=True)\n",
    "    dfL[['pdb_R', 'chain_uniprot_R']] = dfL['R'].str.split('__', expand=True)\n",
    "    dfL[['chain_R', 'uniprot_R']] = dfL['chain_uniprot_R'].str.split('_', expand=True)\n",
    "    dfL[['pdb_L', 'chain_uniprot_L']] = dfL['L'].str.split('__', expand=True)\n",
    "    dfL[['chain_L', 'uniprot_L']] = dfL['chain_uniprot_L'].str.split('_', expand=True)\n",
    "    \n",
    "    # Group by tax_level\n",
    "    negative_ids = []\n",
    "    negative_phyla = []\n",
    "    orig_pinder_ids = set(dfR['pinder_id'])\n",
    "    \n",
    "    for tax_level_el, group in dfR.groupby(f'{tax_level}_R'):\n",
    "        R = group['R'].tolist()\n",
    "        pdb_R = group['pdb_R'].tolist()\n",
    "        uniprot_R = group['uniprot_R'].tolist()\n",
    "\n",
    "        group_L = dfL[dfL[f'{tax_level}_L'] == tax_level_el]\n",
    "        L = group_L['L'].tolist()\n",
    "        pdb_L = group_L['pdb_L'].tolist()\n",
    "        chain_uniprot_L = group_L['chain_uniprot_L'].tolist()\n",
    "        uniprot_L = group_L['uniprot_L'].tolist()\n",
    "\n",
    "        L, pdb_L, chain_uniprot_L, uniprot_L, invalidity_flags = make_random_pairs(\n",
    "            L, pdb_L, chain_uniprot_L, uniprot_L, pdb_R, uniprot_R, \n",
    "            interacting_partners, GO, max_num_attempts\n",
    "        )\n",
    "        # If there are no pairs to form\n",
    "        if(not L): continue\n",
    "        \n",
    "        # If there are invalid pairings, execute removals and get pairs\n",
    "        paired = remove_false_negatives(R, L, uniprot_R, uniprot_L, \n",
    "            interacting_partners, GO, invalidity_flags, orig_pinder_ids\n",
    "        )\n",
    "\n",
    "        negative_ids.extend(paired)\n",
    "        negative_phyla.extend([tax_level_el]*len(paired))\n",
    "\n",
    "    negative_records = list(zip(negative_ids, negative_phyla, negative_phyla))\n",
    "    negative_df = pd.DataFrame(\n",
    "        negative_records, \n",
    "        columns=['pinder_id', f'{tax_level}_R', f'{tax_level}_L']\n",
    "    ).sample(frac=1, random_state=seed).reset_index(drop=True)\n",
    "    \n",
    "    negative_df[['pinder_id_R', 'pinder_id_L']] = negative_df['pinder_id'].str.split('--', expand=True)\n",
    "    \n",
    "    return negative_df\n",
    "\n",
    "def rename_columns(df, to_drop=None, tax_level=\"phylum\"):\n",
    "    df.rename(columns={\n",
    "        \"pinder_id_R\": \"protein1\",\n",
    "        \"pinder_id_L\": \"protein2\",\n",
    "        f\"{tax_level}_R\": f\"{tax_level}1\",\n",
    "        f\"{tax_level}_L\": f\"{tax_level}2\"\n",
    "    }, inplace=True)\n",
    "    if(to_drop): df.drop([to_drop], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_sequences(df):\n",
    "    ids = {\"R\": set(), \"L\": set()}\n",
    "    \n",
    "    for i, j in df.iterrows():\n",
    "        ids[\"R\"].add(df[\"pinder_id_R\"][i])\n",
    "        ids[\"L\"].add(df[\"pinder_id_L\"][i])\n",
    "        \n",
    "    seqs = {}\n",
    "    for k in ids:\n",
    "        for id_ in ids[k]:\n",
    "            struct = structure.Structure(f\"{PINDER_BASE_DIR}/pinder/2024-02/pdbs/{id_}-{k}.pdb\", pinder_id=id_)\n",
    "            seqs[id_] = struct.sequence\n",
    "    return seqs\n",
    "\n",
    "def plot_distributions(dist1, dist2, title, split):\n",
    "    matplotlib.rcParams['figure.dpi'] = 300\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    hist1, _ = np.histogram(dist1, density=True)\n",
    "    hist2, _ = np.histogram(dist2, density=True)\n",
    "    \n",
    "    plt.hist(dist1, color='red', alpha=0.5, label='positive')\n",
    "    plt.hist(dist2, color='blue', alpha=0.5, label='negative')\n",
    "    plt.xlabel('distance')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(f\"{SPLITS_DIR}/{split}.png\", dpi=200)\n",
    "    plt.clf()\n",
    "    \n",
    "    return hist1, hist2\n",
    "\n",
    "def compute_KL_divergence(hist1, hist2):\n",
    "    # Clip to avoid division by zero and ensure valid inputs\n",
    "    epsilon = 1e-10\n",
    "    p1 = np.clip(hist1, epsilon, None)\n",
    "    p2 = np.clip(hist2, epsilon, None)\n",
    "    \n",
    "    # Normalize histograms to make them proper probability distributions\n",
    "    p1 /= p1.sum()\n",
    "    p2 /= p2.sum()\n",
    "    \n",
    "    kl_12 = np.sum(rel_entr(p1, p2))\n",
    "    kl_21 = np.sum(rel_entr(p2, p1))\n",
    "    \n",
    "    return kl_12, kl_21\n",
    "\n",
    "def generate_embeddings(df, seq_df, model, tokenizer, num_samples=5, emb_dim=1280):\n",
    "    embs_R = torch.empty((1, num_samples, emb_dim), dtype=torch.float64)\n",
    "    embs_L = torch.empty((1, num_samples, emb_dim), dtype=torch.float64)\n",
    "\n",
    "    df = df.reset_index()\n",
    "    for i, record in df[:num_samples].iterrows():\n",
    "        enc_R = tokenizer([seq_df[record['pinder_id_R']]], return_tensors='pt').to(DEVICE)\n",
    "        enc_L = tokenizer([seq_df[record['pinder_id_L']]], return_tensors='pt').to(DEVICE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            embs_R[0][i] = model(enc_R.input_ids, output_hidden_states=True).last_hidden_state.mean(dim=1)\n",
    "            embs_L[0][i] = model(enc_L.input_ids, output_hidden_states=True).last_hidden_state.mean(dim=1)\n",
    "            \n",
    "    return torch.squeeze(embs_R), torch.squeeze(embs_L)\n",
    "\n",
    "def make_split_fasta(split):\n",
    "    df = pd.read_csv(f\"{SPLITS_DIR}/{split}.txt\", sep=\"\\t\")\n",
    "\n",
    "    fasta = []\n",
    "    # Reducing duplicates\n",
    "    ids = {\"R\": set(), \"L\": set()}\n",
    "    for i, j in df.iterrows():\n",
    "        idR = df[\"protein1\"][i]\n",
    "        idL = df[\"protein2\"][i]\n",
    "        ids[\"R\"].add(idR)\n",
    "        ids[\"L\"].add(idL)\n",
    "\n",
    "    for k in ids:\n",
    "        for id_ in ids[k]:\n",
    "            struct = structure.Structure(f\"{PINDER_BASE_DIR}/pinder/2024-02/pdbs/{id_}-{k}.pdb\", pinder_id=id_)\n",
    "            fasta.append(SeqRecord(Seq(struct.sequence), id=id_, description=\"\"))\n",
    "    \n",
    "    with open(f\"{SPLITS_DIR}/{split}.fasta\", \"w\") as output_handle:\n",
    "        SeqIO.write(fasta, output_handle, \"fasta-2line\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1ed741-4b89-4eba-838d-9d7614179e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/scicore/home/schwede/durair0000/.local/share/pinder/2024-02')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"PINDER_BASE_DIR\"] = PINDER_BASE_DIR\n",
    "get_pinder_location()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfccd1a2-8e48-4a5d-841d-a3ed3c4e1897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "338759 1799 1767\n",
      "15116 1937 1906\n"
     ]
    }
   ],
   "source": [
    "pinder_ids, uniprot_ids, train_index = select_ids(max_length_per_monomer=512)\n",
    "\n",
    "# Checking number of elements in each split\n",
    "print(len(pinder_ids['train']),\n",
    "      len(pinder_ids['val']),\n",
    "      len(pinder_ids['test'])\n",
    ")\n",
    "\n",
    "print(len(uniprot_ids['train']),\n",
    "      len(uniprot_ids['val']),\n",
    "      len(uniprot_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff0b6dc3-324a-4a82-8018-87a85e3596a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301263 1748 1688\n",
      "15115 1936 1905\n"
     ]
    }
   ],
   "source": [
    "# Selecting that that both defined UniProt IDs\n",
    "pinder_ids['train'] = [row for row in pinder_ids['train'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "pinder_ids['val'] = [row for row in pinder_ids['val'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "pinder_ids['test'] = [row for row in pinder_ids['test'] if not re.search(r'UNDEFINED', row, re.IGNORECASE)]\n",
    "\n",
    "uniprot_ids['train'].remove(\"UNDEFINED\")\n",
    "uniprot_ids['val'].remove(\"UNDEFINED\")\n",
    "uniprot_ids['test'].remove(\"UNDEFINED\")\n",
    "\n",
    "# Checking number of elements in each split after removal of \"UNDEFINED\"\n",
    "print(len(pinder_ids['train']),\n",
    "      len(pinder_ids['val']),\n",
    "      len(pinder_ids['test'])\n",
    ")\n",
    "\n",
    "print(len(uniprot_ids['train']),\n",
    "      len(uniprot_ids['val']),\n",
    "      len(uniprot_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b033f37-8f35-4324-9af7-a9d6e628ac08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save UniProt IDs to a TXT file for \"ID mapping\"\n",
    "with open(UNIPROT_IDS_PATH, 'w') as f:\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        for line in uniprot_ids[split]:\n",
    "            f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ba9739-e7ad-4c40-b513-849c0aaff890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine, which UniProt IDs are belonging to eubacteria with labelled tax_level\n",
    "lineages = pd.read_csv(UNIPROT_LINEAGES_PATH, sep='\\t')\n",
    "\n",
    "eubacteria_phyla = {}\n",
    "eubacteria_annotated_interactions = {}\n",
    "eubacteria_GO = {}\n",
    "\n",
    "for index, row in lineages.iterrows():\n",
    "    if isinstance(row['Taxonomic lineage (Ids)'], str) and \"2 (superkingdom)\" in row['Taxonomic lineage (Ids)']: \n",
    "        match = re.search(r'(\\d+) \\(%s\\)' % TAX_LEVEL, row['Taxonomic lineage (Ids)'])\n",
    "        if match:\n",
    "            tax_level_tax_id = match.group(1)\n",
    "            eubacteria_phyla[row['From']] = tax_level_tax_id\n",
    "            \n",
    "            # Retrieving UniProt IDs of the known interacting partners\n",
    "            if(not pd.isna(row['Interacts with'])):\n",
    "                eubacteria_annotated_interactions[row['From']] = [item.split('-')[0] for item in row['Interacts with'].split('; ')]\n",
    "            else:\n",
    "                eubacteria_annotated_interactions[row['From']] = []\n",
    "            \n",
    "            # Retrieving GO \n",
    "            if(not pd.isna(row['Gene Ontology (molecular function)'])):\n",
    "                eubacteria_GO[row['From']] = re.findall(r\"GO:\\d+\", row['Gene Ontology (molecular function)'])\n",
    "            else:\n",
    "                eubacteria_GO[row['From']] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f23bec9e-4202-4e53-bacc-b5bd185480fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17863 680 645\n"
     ]
    }
   ],
   "source": [
    "# Determine, which PINDER IDs contain both bacterial proteins\n",
    "eubacteria_pinder_ids = {'train': [], 'val': [], 'test': []}\n",
    "eubacteria_pinder_phyla = {}\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    for id in pinder_ids[split]:\n",
    "        uniprot1 = id.split(\"--\")[0].split(\"_\")[-1]\n",
    "        uniprot2 = id.split(\"--\")[1].split(\"_\")[-1]\n",
    "        if(uniprot1 in eubacteria_phyla.keys() and uniprot2 in eubacteria_phyla.keys()):\n",
    "            eubacteria_pinder_ids[split].append(id)\n",
    "            eubacteria_pinder_phyla[id] = [eubacteria_phyla[uniprot1], eubacteria_phyla[uniprot2]]\n",
    "\n",
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74c38009-2642-43b7-8f7c-310da87cb70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17409 680 645\n"
     ]
    }
   ],
   "source": [
    "for split in ['train']:\n",
    "    homodimer_idx = []\n",
    "    for i, id_ in enumerate(eubacteria_pinder_ids[split]):\n",
    "        uniprot1 = id_.split(\"--\")[0].split(\"_\")[-1]\n",
    "        uniprot2 = id_.split(\"--\")[1].split(\"_\")[-1]\n",
    "        if(uniprot1 == uniprot2): homodimer_idx.append(i)\n",
    "\n",
    "    eubacteria_pinder_ids[split] = np.delete(\n",
    "        eubacteria_pinder_ids[split], \n",
    "        homodimer_idx\n",
    "    ) \n",
    "\n",
    "print(len(eubacteria_pinder_ids['train']),\n",
    "      len(eubacteria_pinder_ids['val']),\n",
    "      len(eubacteria_pinder_ids['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b477d8-aa7b-45dd-9e4f-7c90c4e95f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split\tKL(1|0)\tKL(0|1)\n",
      "val\t3.529\t3.936\n",
      "test\t1.321\t1.821\n",
      "17409 8908 680 470 645 454\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 3000x1500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "positives = {}\n",
    "negatives = {}\n",
    "pos_seq = {}\n",
    "neg_seq = {}\n",
    "pos_embs_R = {}\n",
    "pos_embs_L = {}\n",
    "neg_embs_R = {}\n",
    "neg_embs_L = {}\n",
    "pos_dists = {}\n",
    "neg_dists = {}\n",
    "hist_pos = {}\n",
    "hist_neg = {}\n",
    "\n",
    "n_samples = None\n",
    "\n",
    "print(f\"split\\tKL(1|0)\\tKL(0|1)\")\n",
    "\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    positives[split] = get_positives_df(split, eubacteria_pinder_ids, eubacteria_pinder_phyla)\n",
    "    \n",
    "    negatives[split] = make_negative_pairs(positives[split], \n",
    "        positives[\"train\"], eubacteria_annotated_interactions, \n",
    "        eubacteria_GO, tax_level=TAX_LEVEL\n",
    "    )\n",
    "\n",
    "    if(split == 'train'): continue\n",
    "\n",
    "    # Get sequences\n",
    "    pos_seq[split] = get_sequences(positives[split][:n_samples])\n",
    "    neg_seq[split] = get_sequences(negatives[split][:n_samples])\n",
    "\n",
    "    # Get embeddings\n",
    "    pos_embs_R[split], pos_embs_L[split] = generate_embeddings(\n",
    "        positives[split][:n_samples], pos_seq[split], model, tokenizer, \n",
    "        num_samples=len(positives[split][:n_samples]), emb_dim=1280\n",
    "    )\n",
    "    neg_embs_R[split], neg_embs_L[split] = generate_embeddings(\n",
    "        negatives[split][:n_samples], neg_seq[split], model, tokenizer, \n",
    "        num_samples=len(negatives[split][:n_samples]), emb_dim=1280\n",
    "    )\n",
    "\n",
    "    # Compute distances\n",
    "    pos_dists[split] = torch.cdist(\n",
    "        pos_embs_R[split], \n",
    "        pos_embs_L[split]\n",
    "    ).diag()\n",
    "    neg_dists[split] = torch.cdist(\n",
    "        neg_embs_R[split], \n",
    "        neg_embs_L[split]\n",
    "    ).diag()\n",
    "\n",
    "    # Plot distances distribution\n",
    "    hist_pos[split], hist_neg[split] = plot_distributions(\n",
    "        pos_dists[split], neg_dists[split], \n",
    "        f\"Distance distribution of {split} subset\", split\n",
    "    )\n",
    "\n",
    "    # Compute KL divergence\n",
    "    kl10, kl01 = compute_KL_divergence(hist_pos[split], hist_neg[split])\n",
    "    print(f\"{split}\\t{kl10:.3f}\\t{kl01:.3f}\")\n",
    "\n",
    "print(\n",
    "    len(positives['train']),\n",
    "    len(negatives['train']),\n",
    "    len(positives['val']),\n",
    "    len(negatives['val']),\n",
    "    len(positives['test']),\n",
    "    len(negatives['test'])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d06f9d7-6be7-4128-ab34-06659b60977e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split\tClass\tNumber of instances\n",
      "val\t0\t470\n",
      "val\t1\t470\n",
      "test\t0\t454\n",
      "test\t1\t454\n"
     ]
    }
   ],
   "source": [
    "# Save IDs and phyla\n",
    "\n",
    "positives_ = {}\n",
    "negatives_ = {}\n",
    "\n",
    "print(f\"Split\\tClass\\tNumber of instances\")\n",
    "\n",
    "for split in [\"val\", \"test\"]:\n",
    "    positives_[split] = positives[split].sample(n=len(negatives[split]), random_state=SEED)\n",
    "    negatives_[split] = negatives[split]\n",
    "    positives_[split] = positives_[split][[\"pinder_id_R\", \"pinder_id_L\", f\"{TAX_LEVEL}_R\", f\"{TAX_LEVEL}_L\"]]\n",
    "\n",
    "    positives_[split] = rename_columns(positives_[split], tax_level=TAX_LEVEL)\n",
    "    negatives_[split] = rename_columns(negatives_[split], to_drop=\"pinder_id\", tax_level=TAX_LEVEL)\n",
    "    \n",
    "    # Labelling the set\n",
    "    negatives_[split]['label'] = [0]*len(negatives_[split])\n",
    "    positives_[split]['label'] = [1]*len(positives_[split])\n",
    "    \n",
    "    # Summary of the split\n",
    "    print(f\"{split}\\t0\\t{len(negatives_[split])}\\n{split}\\t1\\t{len(positives_[split])}\")\n",
    "\n",
    "    # Merging the classes\n",
    "    all_ = pd.concat([positives_[split], negatives_[split]])\n",
    "    \n",
    "    # Shuffling\n",
    "    all_ = all_.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    \n",
    "    # Saving\n",
    "    all_.to_csv(f\"{SPLITS_DIR}/{split}.txt\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a578c0ec-0e80-4c60-8ef9-de94fb3c5719",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in [\"val\", \"test\"]: make_split_fasta(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40664d89-05e3-4ef9-850b-8d22138310d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!cat $SPLITS_DIR/val.fasta $SPLITS_DIR/test.fasta > $SPLITS_DIR/sequences.fasta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b827ce-df2f-4c6d-bfe1-cf974b4047ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glam",
   "language": "python",
   "name": "glam"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
